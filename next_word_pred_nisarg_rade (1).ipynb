{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\n\n# pickle\nimport pickle\n\n# tf and keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:28.940739Z","iopub.execute_input":"2022-12-07T07:29:28.944659Z","iopub.status.idle":"2022-12-07T07:29:39.207535Z","shell.execute_reply.started":"2022-12-07T07:29:28.944536Z","shell.execute_reply":"2022-12-07T07:29:39.206306Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"file = open(\"/kaggle/input/next-word-prediction-dataset/1661-0.txt\", \"r\", encoding = \"utf8\")\nlines = []\n\nfor i in file:\n    lines.append(i)\n    \ndata = \"\"\n\nfor i in lines:\n    data = ' '. join(lines)\n    \ndata = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“', '').replace(\"”\", '')\n\ndata = data.split()\ndata = ' '.join(data)\ndata[:500]","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:39.209920Z","iopub.execute_input":"2022-12-07T07:29:39.211000Z","iopub.status.idle":"2022-12-07T07:29:47.456541Z","shell.execute_reply.started":"2022-12-07T07:29:39.210957Z","shell.execute_reply":"2022-12-07T07:29:47.450785Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Release Date: November 29, 2002 [EBook #1661] Last Updated: May 20, 2019 Language: English Character set en\""},"metadata":{}}]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:47.457551Z","iopub.execute_input":"2022-12-07T07:29:47.457886Z","iopub.status.idle":"2022-12-07T07:29:47.465706Z","shell.execute_reply.started":"2022-12-07T07:29:47.457853Z","shell.execute_reply":"2022-12-07T07:29:47.464824Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"573660"},"metadata":{}}]},{"cell_type":"code","source":"#preprocessing","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:47.474485Z","iopub.execute_input":"2022-12-07T07:29:47.476450Z","iopub.status.idle":"2022-12-07T07:29:47.500237Z","shell.execute_reply.started":"2022-12-07T07:29:47.476410Z","shell.execute_reply":"2022-12-07T07:29:47.499152Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts([data])\n\npickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n\nsequence_data = tokenizer.texts_to_sequences([data])[0]\nsequence_data[:10]","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:47.501338Z","iopub.execute_input":"2022-12-07T07:29:47.502803Z","iopub.status.idle":"2022-12-07T07:29:47.743404Z","shell.execute_reply.started":"2022-12-07T07:29:47.502687Z","shell.execute_reply":"2022-12-07T07:29:47.741450Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[142, 4680, 1, 986, 5, 125, 33, 46, 556, 2164]"},"metadata":{}}]},{"cell_type":"code","source":"len(sequence_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:47.744758Z","iopub.execute_input":"2022-12-07T07:29:47.745619Z","iopub.status.idle":"2022-12-07T07:29:47.752074Z","shell.execute_reply.started":"2022-12-07T07:29:47.745580Z","shell.execute_reply":"2022-12-07T07:29:47.751223Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"108958"},"metadata":{}}]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:47.753596Z","iopub.execute_input":"2022-12-07T07:29:47.754600Z","iopub.status.idle":"2022-12-07T07:29:47.768150Z","shell.execute_reply.started":"2022-12-07T07:29:47.754543Z","shell.execute_reply":"2022-12-07T07:29:47.767079Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"8624"},"metadata":{}}]},{"cell_type":"code","source":"sequences = []\nfor i in range(3, len(sequence_data)):\n    words = sequence_data[i-3:i+1]\n    sequences.append(words)\n    \nprint(\"The length of sequences :\", len(sequences))\nsequences = np.array(sequences)\nsequences[:10]","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:47.769634Z","iopub.execute_input":"2022-12-07T07:29:47.770175Z","iopub.status.idle":"2022-12-07T07:29:48.072920Z","shell.execute_reply.started":"2022-12-07T07:29:47.770131Z","shell.execute_reply":"2022-12-07T07:29:48.072011Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The length of sequences : 108955\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([[ 142, 4680,    1,  986],\n       [4680,    1,  986,    5],\n       [   1,  986,    5,  125],\n       [ 986,    5,  125,   33],\n       [   5,  125,   33,   46],\n       [ 125,   33,   46,  556],\n       [  33,   46,  556, 2164],\n       [  46,  556, 2164, 2165],\n       [ 556, 2164, 2165,   27],\n       [2164, 2165,   27,  987]])"},"metadata":{}}]},{"cell_type":"markdown","source":"#train and test","metadata":{}},{"cell_type":"code","source":"X = []\ny = []\n\nfor i in sequences:\n    X.append(i[0:3])\n    y.append(i[3])\n    \nX = np.array(X)\ny = np.array(y)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:48.074950Z","iopub.execute_input":"2022-12-07T07:29:48.076017Z","iopub.status.idle":"2022-12-07T07:29:48.248522Z","shell.execute_reply.started":"2022-12-07T07:29:48.075978Z","shell.execute_reply":"2022-12-07T07:29:48.243527Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"Data :\\n\", X[:10])\nprint()\nprint(\"Response :\\n\", y[:10])","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:48.252031Z","iopub.execute_input":"2022-12-07T07:29:48.252820Z","iopub.status.idle":"2022-12-07T07:29:48.260768Z","shell.execute_reply.started":"2022-12-07T07:29:48.252776Z","shell.execute_reply":"2022-12-07T07:29:48.259413Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Data :\n [[ 142 4680    1]\n [4680    1  986]\n [   1  986    5]\n [ 986    5  125]\n [   5  125   33]\n [ 125   33   46]\n [  33   46  556]\n [  46  556 2164]\n [ 556 2164 2165]\n [2164 2165   27]]\n\nResponse :\n [ 986    5  125   33   46  556 2164 2165   27  987]\n","output_type":"stream"}]},{"cell_type":"code","source":"y = to_categorical(y, num_classes=vocab_size)\ny[:10]","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:48.262507Z","iopub.execute_input":"2022-12-07T07:29:48.263665Z","iopub.status.idle":"2022-12-07T07:29:48.545221Z","shell.execute_reply.started":"2022-12-07T07:29:48.263616Z","shell.execute_reply":"2022-12-07T07:29:48.544184Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"#Model Building","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 10, input_length=3))\nmodel.add(LSTM(1000, return_sequences=True))\nmodel.add(LSTM(1000))\nmodel.add(Dense(1000, activation=\"relu\"))\nmodel.add(Dense(vocab_size, activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:48.546655Z","iopub.execute_input":"2022-12-07T07:29:48.547328Z","iopub.status.idle":"2022-12-07T07:29:56.481395Z","shell.execute_reply.started":"2022-12-07T07:29:48.547284Z","shell.execute_reply":"2022-12-07T07:29:56.480360Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2022-12-07 07:29:48.695385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:48.696286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:48.932045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:48.932937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:48.933740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:48.934521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:48.935915: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-07 07:29:49.199067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:49.200048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:49.200869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:49.201598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:49.202306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:49.202999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:53.534725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:53.535591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:53.536336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:53.537132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:53.537854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:53.538503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2022-12-07 07:29:53.542980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-07 07:29:53.543682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:29:56.482676Z","iopub.execute_input":"2022-12-07T07:29:56.483031Z","iopub.status.idle":"2022-12-07T07:29:56.490328Z","shell.execute_reply.started":"2022-12-07T07:29:56.482997Z","shell.execute_reply":"2022-12-07T07:29:56.489332Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 3, 10)             86240     \n_________________________________________________________________\nlstm (LSTM)                  (None, 3, 1000)           4044000   \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 1000)              8004000   \n_________________________________________________________________\ndense (Dense)                (None, 1000)              1001000   \n_________________________________________________________________\ndense_1 (Dense)              (None, 8624)              8632624   \n=================================================================\nTotal params: 21,767,864\nTrainable params: 21,767,864\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1, save_best_only=True)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\nlstm = model.fit(X, y, validation_split=0.2, epochs=100, batch_size=128, shuffle=True, callbacks=[checkpoint]).history","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:31:05.951987Z","iopub.execute_input":"2022-12-07T07:31:05.952966Z","iopub.status.idle":"2022-12-07T07:56:09.106762Z","shell.execute_reply.started":"2022-12-07T07:31:05.952932Z","shell.execute_reply":"2022-12-07T07:56:09.105762Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2022-12-07 07:31:05.966265: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3006809344 exceeds 10% of free system memory.\n2022-12-07 07:31:09.298772: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3006809344 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n681/681 [==============================] - 18s 22ms/step - loss: 5.3716 - val_loss: 6.1649\n\nEpoch 00001: loss improved from inf to 5.37163, saving model to nextword1.h5\nEpoch 2/100\n681/681 [==============================] - 14s 21ms/step - loss: 5.1122 - val_loss: 6.2556\n\nEpoch 00002: loss improved from 5.37163 to 5.11223, saving model to nextword1.h5\nEpoch 3/100\n681/681 [==============================] - 15s 21ms/step - loss: 4.8626 - val_loss: 6.4616\n\nEpoch 00003: loss improved from 5.11223 to 4.86255, saving model to nextword1.h5\nEpoch 4/100\n681/681 [==============================] - 14s 21ms/step - loss: 4.6006 - val_loss: 6.7619\n\nEpoch 00004: loss improved from 4.86255 to 4.60063, saving model to nextword1.h5\nEpoch 5/100\n681/681 [==============================] - 14s 21ms/step - loss: 4.3471 - val_loss: 7.1047\n\nEpoch 00005: loss improved from 4.60063 to 4.34708, saving model to nextword1.h5\nEpoch 6/100\n681/681 [==============================] - 14s 21ms/step - loss: 4.0902 - val_loss: 7.6739\n\nEpoch 00006: loss improved from 4.34708 to 4.09022, saving model to nextword1.h5\nEpoch 7/100\n681/681 [==============================] - 15s 21ms/step - loss: 3.8339 - val_loss: 8.3567\n\nEpoch 00007: loss improved from 4.09022 to 3.83394, saving model to nextword1.h5\nEpoch 8/100\n681/681 [==============================] - 14s 21ms/step - loss: 3.5728 - val_loss: 9.0187\n\nEpoch 00008: loss improved from 3.83394 to 3.57281, saving model to nextword1.h5\nEpoch 9/100\n681/681 [==============================] - 15s 21ms/step - loss: 3.3177 - val_loss: 9.8302\n\nEpoch 00009: loss improved from 3.57281 to 3.31767, saving model to nextword1.h5\nEpoch 10/100\n681/681 [==============================] - 14s 21ms/step - loss: 3.0714 - val_loss: 10.6456\n\nEpoch 00010: loss improved from 3.31767 to 3.07137, saving model to nextword1.h5\nEpoch 11/100\n681/681 [==============================] - 15s 21ms/step - loss: 2.8328 - val_loss: 11.3987\n\nEpoch 00011: loss improved from 3.07137 to 2.83283, saving model to nextword1.h5\nEpoch 12/100\n681/681 [==============================] - 14s 21ms/step - loss: 2.6087 - val_loss: 12.3430\n\nEpoch 00012: loss improved from 2.83283 to 2.60874, saving model to nextword1.h5\nEpoch 13/100\n681/681 [==============================] - 14s 21ms/step - loss: 2.3812 - val_loss: 12.9037\n\nEpoch 00013: loss improved from 2.60874 to 2.38122, saving model to nextword1.h5\nEpoch 14/100\n681/681 [==============================] - 14s 21ms/step - loss: 2.1586 - val_loss: 14.0003\n\nEpoch 00014: loss improved from 2.38122 to 2.15865, saving model to nextword1.h5\nEpoch 15/100\n681/681 [==============================] - 15s 22ms/step - loss: 1.9471 - val_loss: 14.9910\n\nEpoch 00015: loss improved from 2.15865 to 1.94714, saving model to nextword1.h5\nEpoch 16/100\n681/681 [==============================] - 14s 21ms/step - loss: 1.7398 - val_loss: 15.8569\n\nEpoch 00016: loss improved from 1.94714 to 1.73979, saving model to nextword1.h5\nEpoch 17/100\n681/681 [==============================] - 14s 21ms/step - loss: 1.5485 - val_loss: 16.8532\n\nEpoch 00017: loss improved from 1.73979 to 1.54852, saving model to nextword1.h5\nEpoch 18/100\n681/681 [==============================] - 14s 21ms/step - loss: 1.3779 - val_loss: 17.5539\n\nEpoch 00018: loss improved from 1.54852 to 1.37789, saving model to nextword1.h5\nEpoch 19/100\n681/681 [==============================] - 14s 21ms/step - loss: 1.2198 - val_loss: 17.9556\n\nEpoch 00019: loss improved from 1.37789 to 1.21979, saving model to nextword1.h5\nEpoch 20/100\n681/681 [==============================] - 14s 21ms/step - loss: 1.0860 - val_loss: 18.2997\n\nEpoch 00020: loss improved from 1.21979 to 1.08596, saving model to nextword1.h5\nEpoch 21/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.9709 - val_loss: 19.1392\n\nEpoch 00021: loss improved from 1.08596 to 0.97091, saving model to nextword1.h5\nEpoch 22/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.8761 - val_loss: 19.5839\n\nEpoch 00022: loss improved from 0.97091 to 0.87614, saving model to nextword1.h5\nEpoch 23/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.8015 - val_loss: 20.1209\n\nEpoch 00023: loss improved from 0.87614 to 0.80152, saving model to nextword1.h5\nEpoch 24/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.7427 - val_loss: 19.9440\n\nEpoch 00024: loss improved from 0.80152 to 0.74265, saving model to nextword1.h5\nEpoch 25/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.6991 - val_loss: 20.5838\n\nEpoch 00025: loss improved from 0.74265 to 0.69909, saving model to nextword1.h5\nEpoch 26/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.6576 - val_loss: 20.1483\n\nEpoch 00026: loss improved from 0.69909 to 0.65760, saving model to nextword1.h5\nEpoch 27/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.6295 - val_loss: 20.1921\n\nEpoch 00027: loss improved from 0.65760 to 0.62949, saving model to nextword1.h5\nEpoch 28/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.5988 - val_loss: 20.3749\n\nEpoch 00028: loss improved from 0.62949 to 0.59885, saving model to nextword1.h5\nEpoch 29/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.5772 - val_loss: 20.4203\n\nEpoch 00029: loss improved from 0.59885 to 0.57723, saving model to nextword1.h5\nEpoch 30/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.5641 - val_loss: 20.2200\n\nEpoch 00030: loss improved from 0.57723 to 0.56408, saving model to nextword1.h5\nEpoch 31/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.5457 - val_loss: 20.2167\n\nEpoch 00031: loss improved from 0.56408 to 0.54575, saving model to nextword1.h5\nEpoch 32/100\n681/681 [==============================] - 15s 21ms/step - loss: 0.5291 - val_loss: 20.4023\n\nEpoch 00032: loss improved from 0.54575 to 0.52913, saving model to nextword1.h5\nEpoch 33/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.5147 - val_loss: 20.0850\n\nEpoch 00033: loss improved from 0.52913 to 0.51473, saving model to nextword1.h5\nEpoch 34/100\n681/681 [==============================] - 15s 21ms/step - loss: 0.5074 - val_loss: 20.0286\n\nEpoch 00034: loss improved from 0.51473 to 0.50743, saving model to nextword1.h5\nEpoch 35/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.5002 - val_loss: 19.9585\n\nEpoch 00035: loss improved from 0.50743 to 0.50023, saving model to nextword1.h5\nEpoch 36/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4884 - val_loss: 20.2346\n\nEpoch 00036: loss improved from 0.50023 to 0.48841, saving model to nextword1.h5\nEpoch 37/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4766 - val_loss: 20.0012\n\nEpoch 00037: loss improved from 0.48841 to 0.47664, saving model to nextword1.h5\nEpoch 38/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4695 - val_loss: 19.9805\n\nEpoch 00038: loss improved from 0.47664 to 0.46948, saving model to nextword1.h5\nEpoch 39/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4602 - val_loss: 20.2438\n\nEpoch 00039: loss improved from 0.46948 to 0.46022, saving model to nextword1.h5\nEpoch 40/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4555 - val_loss: 19.5682\n\nEpoch 00040: loss improved from 0.46022 to 0.45552, saving model to nextword1.h5\nEpoch 41/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4498 - val_loss: 19.9229\n\nEpoch 00041: loss improved from 0.45552 to 0.44980, saving model to nextword1.h5\nEpoch 42/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4434 - val_loss: 19.8777\n\nEpoch 00042: loss improved from 0.44980 to 0.44338, saving model to nextword1.h5\nEpoch 43/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4374 - val_loss: 20.0157\n\nEpoch 00043: loss improved from 0.44338 to 0.43742, saving model to nextword1.h5\nEpoch 44/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4311 - val_loss: 19.9158\n\nEpoch 00044: loss improved from 0.43742 to 0.43108, saving model to nextword1.h5\nEpoch 45/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4254 - val_loss: 19.8385\n\nEpoch 00045: loss improved from 0.43108 to 0.42536, saving model to nextword1.h5\nEpoch 46/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4193 - val_loss: 19.6026\n\nEpoch 00046: loss improved from 0.42536 to 0.41932, saving model to nextword1.h5\nEpoch 47/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4142 - val_loss: 19.8979\n\nEpoch 00047: loss improved from 0.41932 to 0.41418, saving model to nextword1.h5\nEpoch 48/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4116 - val_loss: 20.2417\n\nEpoch 00048: loss improved from 0.41418 to 0.41165, saving model to nextword1.h5\nEpoch 49/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4133 - val_loss: 19.9474\n\nEpoch 00049: loss did not improve from 0.41165\nEpoch 50/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.4093 - val_loss: 19.8713\n\nEpoch 00050: loss improved from 0.41165 to 0.40932, saving model to nextword1.h5\nEpoch 51/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3971 - val_loss: 20.3256\n\nEpoch 00051: loss improved from 0.40932 to 0.39709, saving model to nextword1.h5\nEpoch 52/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3922 - val_loss: 20.0654\n\nEpoch 00052: loss improved from 0.39709 to 0.39220, saving model to nextword1.h5\nEpoch 53/100\n681/681 [==============================] - 15s 21ms/step - loss: 0.3896 - val_loss: 20.5389\n\nEpoch 00053: loss improved from 0.39220 to 0.38963, saving model to nextword1.h5\nEpoch 54/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3884 - val_loss: 20.1509\n\nEpoch 00054: loss improved from 0.38963 to 0.38844, saving model to nextword1.h5\nEpoch 55/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3841 - val_loss: 19.8641\n\nEpoch 00055: loss improved from 0.38844 to 0.38414, saving model to nextword1.h5\nEpoch 56/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3819 - val_loss: 20.3746\n\nEpoch 00056: loss improved from 0.38414 to 0.38187, saving model to nextword1.h5\nEpoch 57/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3824 - val_loss: 20.0920\n\nEpoch 00057: loss did not improve from 0.38187\nEpoch 58/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3786 - val_loss: 20.5136\n\nEpoch 00058: loss improved from 0.38187 to 0.37859, saving model to nextword1.h5\nEpoch 59/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3770 - val_loss: 20.2639\n\nEpoch 00059: loss improved from 0.37859 to 0.37699, saving model to nextword1.h5\nEpoch 60/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3697 - val_loss: 21.0417\n\nEpoch 00060: loss improved from 0.37699 to 0.36971, saving model to nextword1.h5\nEpoch 61/100\n681/681 [==============================] - 15s 21ms/step - loss: 0.3661 - val_loss: 20.0486\n\nEpoch 00061: loss improved from 0.36971 to 0.36611, saving model to nextword1.h5\nEpoch 62/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3633 - val_loss: 19.8908\n\nEpoch 00062: loss improved from 0.36611 to 0.36335, saving model to nextword1.h5\nEpoch 63/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3623 - val_loss: 20.4807\n\nEpoch 00063: loss improved from 0.36335 to 0.36233, saving model to nextword1.h5\nEpoch 64/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3651 - val_loss: 20.5912\n\nEpoch 00064: loss did not improve from 0.36233\nEpoch 65/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3584 - val_loss: 20.2484\n\nEpoch 00065: loss improved from 0.36233 to 0.35842, saving model to nextword1.h5\nEpoch 66/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3570 - val_loss: 20.7295\n\nEpoch 00066: loss improved from 0.35842 to 0.35699, saving model to nextword1.h5\nEpoch 67/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3565 - val_loss: 20.8397\n\nEpoch 00067: loss improved from 0.35699 to 0.35654, saving model to nextword1.h5\nEpoch 68/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3521 - val_loss: 20.6872\n\nEpoch 00068: loss improved from 0.35654 to 0.35211, saving model to nextword1.h5\nEpoch 69/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3478 - val_loss: 20.7015\n\nEpoch 00069: loss improved from 0.35211 to 0.34783, saving model to nextword1.h5\nEpoch 70/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3485 - val_loss: 20.7465\n\nEpoch 00070: loss did not improve from 0.34783\nEpoch 71/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3489 - val_loss: 20.9348\n\nEpoch 00071: loss did not improve from 0.34783\nEpoch 72/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3489 - val_loss: 20.8871\n\nEpoch 00072: loss did not improve from 0.34783\nEpoch 73/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3446 - val_loss: 20.5526\n\nEpoch 00073: loss improved from 0.34783 to 0.34463, saving model to nextword1.h5\nEpoch 74/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3363 - val_loss: 20.8987\n\nEpoch 00074: loss improved from 0.34463 to 0.33626, saving model to nextword1.h5\nEpoch 75/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3379 - val_loss: 21.2235\n\nEpoch 00075: loss did not improve from 0.33626\nEpoch 76/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3341 - val_loss: 20.8479\n\nEpoch 00076: loss improved from 0.33626 to 0.33409, saving model to nextword1.h5\nEpoch 77/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3334 - val_loss: 21.0425\n\nEpoch 00077: loss improved from 0.33409 to 0.33337, saving model to nextword1.h5\nEpoch 78/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3374 - val_loss: 20.9393\n\nEpoch 00078: loss did not improve from 0.33337\nEpoch 79/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3387 - val_loss: 21.0227\n\nEpoch 00079: loss did not improve from 0.33337\nEpoch 80/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3303 - val_loss: 21.2882\n\nEpoch 00080: loss improved from 0.33337 to 0.33032, saving model to nextword1.h5\nEpoch 81/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3303 - val_loss: 20.7684\n\nEpoch 00081: loss did not improve from 0.33032\nEpoch 82/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3310 - val_loss: 21.2052\n\nEpoch 00082: loss did not improve from 0.33032\nEpoch 83/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3258 - val_loss: 21.2454\n\nEpoch 00083: loss improved from 0.33032 to 0.32576, saving model to nextword1.h5\nEpoch 84/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3256 - val_loss: 21.4280\n\nEpoch 00084: loss improved from 0.32576 to 0.32556, saving model to nextword1.h5\nEpoch 85/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3295 - val_loss: 21.9100\n\nEpoch 00085: loss did not improve from 0.32556\nEpoch 86/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3267 - val_loss: 20.7684\n\nEpoch 00086: loss did not improve from 0.32556\nEpoch 87/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3221 - val_loss: 21.3595\n\nEpoch 00087: loss improved from 0.32556 to 0.32214, saving model to nextword1.h5\nEpoch 88/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3198 - val_loss: 21.3847\n\nEpoch 00088: loss improved from 0.32214 to 0.31975, saving model to nextword1.h5\nEpoch 89/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3181 - val_loss: 20.9573\n\nEpoch 00089: loss improved from 0.31975 to 0.31806, saving model to nextword1.h5\nEpoch 90/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3138 - val_loss: 21.7951\n\nEpoch 00090: loss improved from 0.31806 to 0.31382, saving model to nextword1.h5\nEpoch 91/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3166 - val_loss: 21.2197\n\nEpoch 00091: loss did not improve from 0.31382\nEpoch 92/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3282 - val_loss: 21.6462\n\nEpoch 00092: loss did not improve from 0.31382\nEpoch 93/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3305 - val_loss: 21.8451\n\nEpoch 00093: loss did not improve from 0.31382\nEpoch 94/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3163 - val_loss: 22.1992\n\nEpoch 00094: loss did not improve from 0.31382\nEpoch 95/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3099 - val_loss: 22.2396\n\nEpoch 00095: loss improved from 0.31382 to 0.30986, saving model to nextword1.h5\nEpoch 96/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3025 - val_loss: 21.3904\n\nEpoch 00096: loss improved from 0.30986 to 0.30248, saving model to nextword1.h5\nEpoch 97/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3039 - val_loss: 21.8231\n\nEpoch 00097: loss did not improve from 0.30248\nEpoch 98/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3148 - val_loss: 21.9523\n\nEpoch 00098: loss did not improve from 0.30248\nEpoch 99/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3192 - val_loss: 22.1996\n\nEpoch 00099: loss did not improve from 0.30248\nEpoch 100/100\n681/681 [==============================] - 14s 21ms/step - loss: 0.3118 - val_loss: 21.8631\n\nEpoch 00100: loss did not improve from 0.30248\n","output_type":"stream"}]},{"cell_type":"code","source":"model = load_model(\"nextword1.h5\")\ntokenizer = pickle.load(open(\"tokenizer1.pkl\", 'rb'))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:56:09.113165Z","iopub.execute_input":"2022-12-07T07:56:09.113480Z","iopub.status.idle":"2022-12-07T07:56:11.991974Z","shell.execute_reply.started":"2022-12-07T07:56:09.113452Z","shell.execute_reply":"2022-12-07T07:56:11.990966Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def predict_next_words(model, tokenizer, text) :\n    \n    sequence = tokenizer.texts_to_sequences([text])\n    sequence = np.array(sequence)\n    \n    preds = np.argmax(model.predict(sequence))\n    predicted_word = \"\"\n    \n    for key, value in tokenizer.word_index.items() :\n        if value == preds :\n            predicted_word = key\n            break\n    \n    return predicted_word","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:56:11.993650Z","iopub.execute_input":"2022-12-07T07:56:11.994054Z","iopub.status.idle":"2022-12-07T07:56:12.002758Z","shell.execute_reply.started":"2022-12-07T07:56:11.994018Z","shell.execute_reply":"2022-12-07T07:56:12.001833Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"l = [\"The Adventures of\", \"by Arthur Conan\", \"it is all\",\"you will be\", \"0\"]\n\nfor i in l :\n    \n    text = i\n    \n    if text == \"0\" :\n        print()\n        print(\"Execution completed...\")\n        break\n    \n    else :\n        try :\n            text = text.split(\" \")\n            text = text[-3:]\n            print(\"Input :\", i)\n            print(\"Output : \", i, predict_next_words(model, tokenizer, text))\n            print()\n            \n        except Exception as e :\n            print(\"Error occured : \", e)\n            continue","metadata":{"execution":{"iopub.status.busy":"2022-12-07T07:58:51.798574Z","iopub.execute_input":"2022-12-07T07:58:51.798938Z","iopub.status.idle":"2022-12-07T07:58:51.956784Z","shell.execute_reply.started":"2022-12-07T07:58:51.798908Z","shell.execute_reply":"2022-12-07T07:58:51.955775Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Input : The Adventures of\nOutput :  The Adventures of sherlock\n\nInput : by Arthur Conan\nOutput :  by Arthur Conan doyle\n\nInput : it is all\nOutput :  it is all right\n\nInput : you will be\nOutput :  you will be good\n\n\nExecution completed...\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}